{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project 2 - digit recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "h86s36VU1PZt",
        "dVGyzV0S6nTQ",
        "MnzpCDlvFkMY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEWnY-0m5A-Y"
      },
      "source": [
        "**NOTE:** \n",
        "- the complete execution of this notebook takes about 20 hours on Google Colab\n",
        "- the train test split is deliberately included in each cell so that they can be executed independently\n",
        "- I did not define a lot of functions because each cell is only about 60 lines long and defining functions in this case makes is harder to follow the data flow and in my opinion decreases readability, adding cognitive overhead\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Rxy-7yO0hi"
      },
      "source": [
        "#Imports and function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV4_BtYwy5X_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "from sklearn import neighbors\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import scale, StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "# Fix the seed for reproducibility:\n",
        "np.random.seed(666)\n",
        "set_seed(666)\n",
        "\n",
        "### Normalize features, transform labels into a one-hot encoding vector:\n",
        "def preprocess_data(X, y, num_classes):\n",
        "    scaledX = X/255\n",
        "    oneHoty = to_categorical(y, num_classes=num_classes)\n",
        "    return scaledX, oneHoty\n",
        "\n",
        "def build_MLP(num_classes, nodes, layers, activation_input='relu'):\n",
        "    MLP = Sequential()\n",
        "    MLP.add(Flatten())\n",
        "    # Hidden layers (fully connected):\n",
        "    for i in range(0,layers):\n",
        "        MLP.add(Dense(nodes, activation=activation_input))\n",
        "\n",
        "    # Output layer (fully-connected):\n",
        "    MLP.add(Dense(num_classes, activation='softmax'))\n",
        "    MLP.compile(loss=categorical_crossentropy,\n",
        "                optimizer='SGD',\n",
        "                metrics='accuracy')\n",
        "    return MLP\n",
        "\n",
        "### Split+shuffle X and Y into k=num_folds different folds:\n",
        "def KFold_split(X, Y, num_folds, seed):\n",
        "    KFold_splitter = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
        "    X_train_folds = []\n",
        "    X_val_folds = []\n",
        "    Y_train_folds = []\n",
        "    Y_val_folds = []\n",
        "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X, Y):\n",
        "        X_train_folds.append(X[kth_fold_train_idxs])\n",
        "        X_val_folds.append(X[kth_fold_val_idxs])\n",
        "        Y_train_folds.append(Y[kth_fold_train_idxs])\n",
        "        Y_val_folds.append(Y[kth_fold_val_idxs])\n",
        "    return X_train_folds, X_val_folds, Y_train_folds, Y_val_folds"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMhD3PFkzH5X"
      },
      "source": [
        "#Data preprocessing\n",
        "\n",
        "- display some meta information about the dataset\n",
        "- display some of the images used for classification for a general impression of the data worked with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3jEdIYCpdza"
      },
      "source": [
        "#read the data from the csv\n",
        "dataX = pd.read_csv(\"handwritten_digits_images.csv\")\n",
        "print(\"INPUT DATA\")\n",
        "dataX.info()\n",
        "dataX = dataX.to_numpy()\n",
        "X = dataX.reshape(dataX.shape[0], 28, 28);\n",
        "\n",
        "print()\n",
        "print(\"LABELS\")\n",
        "dataY = pd.read_csv(\"handwritten_digits_labels.csv\")\n",
        "dataY.info()\n",
        "dataY = dataY.to_numpy()\n",
        "y = dataY.reshape(-1,1)\n",
        "\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "#scale the data, one-hot-encode the labels\n",
        "X, y = preprocess_data(X, y, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvIvZ7DcvXHd"
      },
      "source": [
        "#display a random selection of the images with their correct label\n",
        "\n",
        "#reverse the one-hot encoding\n",
        "y_integers = np.argmax(y, axis=1)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y_integers, random_state=221, test_size=0.01, shuffle=True)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "for i in range(len(X_test)):\n",
        "    #each image gets a subplot\n",
        "    ax = fig.add_subplot(28, 25, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(X_test[i], cmap='Greys')\n",
        "    #add the correct label as text to the subplot\n",
        "    ax.text(0, 8, str(Y_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H6T55ttn6G"
      },
      "source": [
        "#Model Selection\n",
        "\n",
        "- each type of classifier is tested on the validation data to find the best-performing parameters (automated)\n",
        "- then the performance of this best performing configuration of each classfiers is compared to the others to find out which one the overall best performing classifier on the validation dataset is (manually)\n",
        "- the same random states during train test split assure that the same data is used during all steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AHqne3G1J-5"
      },
      "source": [
        "##Basic Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS406z0-0i51"
      },
      "source": [
        "#requires one-hot encoding\n",
        "input_shape = np.shape(X)\n",
        "\n",
        "#split into train and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=221, test_size=0.4, shuffle=True)\n",
        "#extract train and validation folds:\n",
        "X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train, Y_train, num_folds=3, seed=221)\n",
        "\n",
        "#model selection parameters\n",
        "train_batch_size = [500, 1000]\n",
        "nodes = [392,784]\n",
        "layers = [1,2,3]\n",
        "\n",
        "configs = []\n",
        "accuracies = []\n",
        "\n",
        "for tbs in train_batch_size:\n",
        "    for node in nodes:\n",
        "        for layer in layers:\n",
        "            fold_accuracies = []\n",
        "            #test the configuration on three folds of the training data\n",
        "            for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
        "                #setup the model with the current configuration\n",
        "                model = build_MLP(num_classes, node, layer, activation_input='relu')\n",
        "                #build and train the model\n",
        "                model.build(input_shape)\n",
        "                model.fit(X_train_fold, Y_train_fold, validation_split=0.3, batch_size=tbs, epochs=50, verbose=0)\n",
        "                #evaluate\n",
        "                test_results = model.evaluate(X_val_fold, Y_val_fold, verbose=0)\n",
        "                fold_accuracies.append(test_results[1])\n",
        "            #save the average accuracy achieved and the corresponding configuration\n",
        "            configs.append((tbs, node, layer))\n",
        "            print(\"======\")\n",
        "            print(\"Configuration: \" + str((tbs, node, layer)))\n",
        "            print(\"Accuracy: \" + str(fold_accuracies) + \" Average: \" + str(mean(fold_accuracies)))\n",
        "            accuracies.append(mean(fold_accuracies))\n",
        "\n",
        "#print the best-performing configuration\n",
        "max_config_indx = accuracies.index(max(accuracies))\n",
        "print()\n",
        "print(\"=============\")\n",
        "print(\"best-performing configuration:\")\n",
        "print(configs[max_config_indx])\n",
        "print(max(accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h86s36VU1PZt"
      },
      "source": [
        "##Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0eKPcC21Rpr"
      },
      "source": [
        "#flatten input, one-hot encoding \n",
        "X = X.reshape(69999,784)\n",
        "#split into train and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=221, test_size=0.4, shuffle=True)\n",
        "#extract train and validation folds:\n",
        "X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train, Y_train, num_folds=3, seed=221)\n",
        "\n",
        "#model selection parameters\n",
        "criterions = ['gini', 'entropy']\n",
        "splitters = ['best', 'random']\n",
        "\n",
        "configs = []\n",
        "accuracies = []\n",
        "\n",
        "for crit in criterions:\n",
        "    for split in splitters:\n",
        "        fold_accuracies = []\n",
        "        #test the configuration on three folds of the training data\n",
        "        for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
        "            #setup the classifier with the current configuration\n",
        "            clf = DecisionTreeClassifier(criterion=crit, splitter=split, random_state=0)\n",
        "            #train\n",
        "            clf.fit(X_train_fold, Y_train_fold)\n",
        "            #predict\n",
        "            Y_pred = clf.predict(X_val_fold)\n",
        "            #evaluate\n",
        "            accu = -1000 #in case something doesn't work (will be noticed during inspection of the results)\n",
        "            accu = accuracy_score(Y_val_fold, Y_pred)\n",
        "            fold_accuracies.append(accu)\n",
        "        #save the average accuracy achieved and the corresponding configuration\n",
        "        configs.append((crit, split))\n",
        "        print(\"======\")\n",
        "        print(\"Configuration: \" + str((crit, split)))\n",
        "        print(\"Accuracy: \" + str(fold_accuracies) + \" Average: \" + str(mean(fold_accuracies)))\n",
        "        accuracies.append(mean(fold_accuracies))\n",
        "\n",
        "#print the best-performing configuration\n",
        "max_config_indx = accuracies.index(max(accuracies))\n",
        "print()\n",
        "print(\"=============\")\n",
        "print(\"best-performing configuration:\")\n",
        "print(configs[max_config_indx])\n",
        "print(max(accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVGyzV0S6nTQ"
      },
      "source": [
        "##kNN-Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L39CHSPy7Fcr"
      },
      "source": [
        "#reverse one-hot-encoding, flatten input\n",
        "X_flattened = X.reshape(69999,784)\n",
        "y_integers = np.argmax(y, axis=1)\n",
        "#split into train and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_flattened, y_integers, random_state=221, test_size=0.4, shuffle=True)\n",
        "#extract train and validation folds:\n",
        "X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train, Y_train, num_folds=3, seed=221)\n",
        "\n",
        "#model selection parameters\n",
        "ks = [32, 64, 512, 1024]\n",
        "weights = ['uniform', 'distance']\n",
        "\n",
        "configs = []\n",
        "accuracies = []\n",
        "\n",
        "for k in ks:\n",
        "    for w in weights:\n",
        "        fold_accuracies = []\n",
        "        #test the configuration on three folds of the training data\n",
        "        for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
        "            #setup classifier with the current configuration\n",
        "            k_NN = neighbors.KNeighborsClassifier(k, weights=w)\n",
        "            #train\n",
        "            k_NN.fit(X_train_fold,Y_train_fold) \n",
        "            #predict\n",
        "            Y_pred = k_NN.predict(X_val_fold)\n",
        "            #evaluate\n",
        "            accu = -1000 #in case something doesn't work (will be noticed during inspection of the results)\n",
        "            accu = accuracy_score(Y_val_fold, Y_pred)\n",
        "            fold_accuracies.append(accu)\n",
        "        #save the average accuracy achieved and the corresponding configuration\n",
        "        configs.append((k,w)) \n",
        "        accuracies.append(mean(fold_accuracies))\n",
        "        print(\"======\")\n",
        "        print(\"Configuration: \" + str((k,w)))\n",
        "        print(\"Accuracy: \" + str(fold_accuracies) + \" Average: \" + str(mean(fold_accuracies)))\n",
        "\n",
        "#print the best-performing configuration\n",
        "max_config_indx = accuracies.index(max(accuracies))\n",
        "print()\n",
        "print(\"=============\")\n",
        "print(\"best-performing configuration:\")\n",
        "print(configs[max_config_indx])\n",
        "print(max(accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnzpCDlvFkMY"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0M1ORmZFmBv"
      },
      "source": [
        "#reverse one-hot-encoding, flatten input\n",
        "X_flattened = X.reshape(69999,784)\n",
        "y_integers = np.argmax(y, axis=1)\n",
        "\n",
        "#split into train and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_flattened, y_integers, random_state=221, test_size=0.4, shuffle=True)\n",
        "#extract train and validation folds:\n",
        "X_train_folds, X_val_folds, Y_train_folds, Y_val_folds = KFold_split(X_train, Y_train, num_folds=3, seed=221)\n",
        "\n",
        "#model selection parameters\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "degrees = [2,3] #only significant in poly\n",
        "coef0s = [0,1,2] #only significant in poly\n",
        "gammas = ['scale', 'auto'] #ignored for linear kernel\n",
        "C = [1,2,3]\n",
        "\n",
        "configs = []\n",
        "accuracies = []\n",
        "\n",
        "for gamma in gammas:\n",
        "    for kernel in kernels:\n",
        "        for deg in degrees:\n",
        "            for coef in coef0s:\n",
        "                for c in C:\n",
        "                    fold_accuracies = []\n",
        "                    if kernel != 'poly' and deg != 2:\n",
        "                        pass #degree is only significant for poly kernel\n",
        "                    elif kernel != 'poly' and coef != 0:\n",
        "                        pass #coef0 is only significant for poly kernel\n",
        "                    elif kernel == 'linear' and gamma != 'auto': \n",
        "                        pass #gamma parameter is not significant for linear kernel\n",
        "                    if gamma != 'auto' or coef != 2 or deg != 3 or kernel != 'poly' or c != 2:\n",
        "                        pass\n",
        "                    else:\n",
        "                        #test the configuration on three folds of the training data\n",
        "                        for X_train_fold, X_val_fold, Y_train_fold, Y_val_fold in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
        "                                #setup classifier with the current configuration\n",
        "                                clf = SVC(decision_function_shape='ovo', kernel=kernel, gamma=gamma, coef0=coef, degree=deg, C=c)\n",
        "                                #train\n",
        "                                clf.fit(X_train_fold, Y_train_fold)\n",
        "                                #predict\n",
        "                                Y_pred = clf.predict(X_val_fold)\n",
        "                                #evaluate\n",
        "                                accu = -1000 #in case something doesn't work (will be noticed during inspection of the results)\n",
        "                                accu = accuracy_score(Y_val_fold, Y_pred)\n",
        "                                fold_accuracies.append(accu)\n",
        "                        #save the average accuracy achieved and the corresponding configuration\n",
        "                        configs.append((gamma, coef, deg, kernel, c))\n",
        "                        print(\"======\")\n",
        "                        print(\"Configuration: \" + str((gamma, coef, deg, kernel, c)))\n",
        "                        print(\"Accuracy: \" + str(fold_accuracies) + \" Average: \" + str(mean(fold_accuracies)))\n",
        "                        accuracies.append(mean(fold_accuracies))\n",
        "\n",
        "#print the best-performing configuration\n",
        "max_config_indx = accuracies.index(max(accuracies))\n",
        "print()\n",
        "print(\"=============\")\n",
        "print(\"best-performing configuration:\")\n",
        "print(configs[max_config_indx])\n",
        "print(max(accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z99tBTSCtyfV"
      },
      "source": [
        "#Evaluation\n",
        "\n",
        "- below the best performing classifier from all of the above is evaluated on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESk7zSxZt0_d"
      },
      "source": [
        "#reverse one-hot-encoding, flatten input\n",
        "X_flattened = X.reshape(69999,784)\n",
        "y_integers = np.argmax(y, axis=1)\n",
        "\n",
        "#split into train and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_flattened, y_integers, random_state=221, test_size=0.4, shuffle=True)\n",
        "\n",
        "#setup classifier with the best-performing configuration\n",
        "clf = SVC(decision_function_shape='ovo', kernel='rbf', gamma='scale', C=3)\n",
        "#train\n",
        "clf.fit(X_train, Y_train)\n",
        "#predict\n",
        "Y_pred = clf.predict(X_test)\n",
        "#evaluate\n",
        "print(\"Accuracy: \", accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}